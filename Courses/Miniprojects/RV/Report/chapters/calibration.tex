\chapter{Calibration}

\section{From image to camera}
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/camera_calibration_focal_point.png}
\caption{}
\label{fig:camera_calibration_focal_point}
\end{figure}
When taking an image for the use of detecting an object it is necessary to transform the image frame into the robot frame. First the image has to be transformed from world coordinates into camera coordinates and then to robot coordinates. When going from world to camera the pinhole camera model can be used. A graphical depiction of the model is shown in \autoref{fig:camera_calibration_focal_point}. The camera is assumed to be a box where no light can enter except from a small hole called the focal point. The light that is cast on the back wall is an inverted 2D representation of the 3D world outside of the box. The 2D image depends on the distance from the focal point to the back wall, called the focal length. The transformation is split into two parts; from world coordinates to camera coordinates and from camera coordinates to image coordinates. This is done by finding the intrinsic and extrinsic parameters. The intrinsic parameters look like shown in \autoref{eq:intr_param}.

\begin{equation}\label{eq:intr_param}
\begin{bmatrix}
u \\
v \\
w 
\end{bmatrix} 
 =
\begin{bmatrix}
a_x & s & x_0 & 0 \\
0 & a_y & y_0 & 0 \\
0 & 0 & 1 & 0 
\end{bmatrix}  
\begin{bmatrix}
x_s \\
y_s \\
z_s \\
0 
\end{bmatrix}
\end{equation}

where $x_s, ~y_s, ~z_s$ are the scene points in the camera coordinate system, $u, v, w$ are the homogeneous image coordinates where $w$ is a scaling factor. The parameters are $a_x, a_y$ which are the focal length. $x_0$ and $y_0$ are to translate the image centre to be the actual image centre since there may be an offset due to fabrication issues.  $s$ is the pixel skew parameter but it is usually set to 0.

The extrinsic parameters are a translation and rotation that describe where in the world the camera is located and how it is oriented. The parameters can be expressed as:
$ \begin{bmatrix}
R & -RC  \\
0_{3}^{T} & 1 
\end{bmatrix}  $
where $R$ is the rotational $3\times 3$ matrix and $-RC$ is the $3$dimensional vector translation. To get the whole transformation it would look like shown in \autoref{eq:cam_trans}.

\begin{equation}\label{eq:cam_trans}
\begin{bmatrix}
u \\
v \\
w 
\end{bmatrix} 
 =
\begin{bmatrix}
a_x & s & x_0 & 0 \\
0 & a_y & y_0 & 0 \\
0 & 0 & 1 & 0 
\end{bmatrix}  
\begin{bmatrix}
R & -RC  \\
0_{3}^{T} & 1 
\end{bmatrix}
\begin{bmatrix}
x_s \\
y_s \\
z_s \\
0 
\end{bmatrix}
\end{equation}

Lastly there can also be some radial distortion in the image caused by the lens, which causes lines that are linear in the world coordinate to appear non-linear in the image. This correction can be computed by using images with objects with known straight lines. To get all these parameters the Camera Calibration Toolbox for Matlab by Jean-Yves Bouguet can be used.  Here 10 to 20 images are taken of a checker board pattern as shown in \autoref{fig:checkerbord_pattern}, where the size of the squares is known, and from this it is possible to estimate the intrinsic and extrinsic parameters and distortion of the camera.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/checkerbord_pattern.png}
\caption{}
\label{fig:checkerbord_pattern}
\end{figure}


\section{From image to robot}
Once the parameters are known they need to be transformed into robot coordinates. For this the transformation matrix from the camera coordinates to robot is needed. A way to simplify this is to use a different approach than finding the camera parameters. If the camera is orthogonal to the planar space that the robot needs to operate in it can be assumed that image is simply a version of the planar space with constant scaling. If a point is known in pixel coordinates and in robot coordinates it is possible to compute a simple projection between the points. Then it is not necessary to get the camera parameters and know the robots frame as long as the mapping is known. The mapping can be found as shown in \autoref{fig:camera_robot_shortcut} with \autoref{eq:cam_calib1}.

\begin{equation}\label{eq:cam_calib1}
\begin{split}
x_{rob}&=\theta_{0}+\theta_{1}*x_{img}+\theta_{2}*y_{img}\\
x_{rob}&=(1+*x_{img}+y_{img}) * \vec{\theta}
\end{split}
\end{equation}

Where $x_{rob}$ is the $ x $ coordinate for the robot in a place in the image space. $x_{img}$ and $y_{img}$ are the pixel coordinates of the exact same point with $\theta_1$ and $\theta_2$ as scaling factors for when the robot moves in the $ x $ direction in robot coordinates how much does it move in $ x $ and $ y $ of the pixel coordinates. $\theta_0$ is the offset that comes from the robot's base not having the same origin as the image. When done for both $ x $ and $ y $ the result is found using \autoref{eq:cam_calib2}
 
\begin{equation}\label{eq:cam_calib}
\begin{split}
x_{rob}&=(1+*x_{img}+y_{img})*\vec{\theta}\\
y_{rob}&=(1+*x_{img}+y_{img}) * \vec{\phi}
\end{split}
\end{equation}

Where $\theta$ is the $ x $ parameters and $\phi$ is the $ y $ parameters. These can be solved for by using multiple known points and their accompanying robot and pixel coordinates like shown in \autoref{eq:rob_x}

\begin{equation}\label{eq:rob_pix}
\begin{bmatrix}
x_{rob1} \\
x_{rob2} \\
x_{rob3}
\end{bmatrix} 
 =
\begin{bmatrix}
1 & x_{img1} & y_{img1} \\
1 & x_{img2} & y_{img2} \\
1 & x_{img3} & y_{img3} 
\end{bmatrix}  
* \vec{\theta}
\end{equation}

To then go to robot coordinates from pixel coordinates \autoref{eq:rob_coord} is used.

\begin{equation}\label{eq:rob_coord}
\begin{bmatrix}
x_{rob} \\
y_{rob} \\
\end{bmatrix} 
= 
\begin{bmatrix}
1 & x_{img} & y_{img}\\
\end{bmatrix} 
\begin{bmatrix}
\theta_0 & \phi_0\\
\theta_1 & \phi_1\\
\theta_2 & \phi_3  
\end{bmatrix}  
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/camera_robot_shortcut.png}
\caption{}
\label{fig:camera_robot_shortcut}
\end{figure}

This procedure will increase in precision with the increasing number of points used. In the project it is chosen to use three stationary points in the workspace frame of the operating table.
